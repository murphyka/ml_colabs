{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poking around at pre-trained computer vision models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUDn-l3v5hj7"
      },
      "source": [
        "In this colab we'll download two pre-trained image classification networks and dig into them a little."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qowiOZM65dGf"
      },
      "source": [
        "!pip install transformers datasets\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow_datasets as tfds\n",
        "import scipy.ndimage as nim\n",
        "from transformers import AutoFeatureExtractor, ViTFeatureExtractor, ViTModel, ViTConfig\n",
        "from PIL import Image\n",
        "import requests\n",
        "tfkl = tf.keras.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's download some pics and resize them to match the 224x224 \n",
        "## standard dimensions.  All grabbed from unsplash.com\n",
        "\n",
        "## Feel free to change these with your own urls/uploaded images\n",
        "urls = [\"https://source.unsplash.com/lylCw4zcA7I\",\n",
        "        \"https://source.unsplash.com/QJ2HGuSSQz0\",\n",
        "        \"https://source.unsplash.com/p7tai9P7H-s\",\n",
        "        \"https://source.unsplash.com/5U_28ojjgms\",\n",
        "        \"https://source.unsplash.com/Gk8LG7dsHWA\",\n",
        "        \"https://source.unsplash.com/1Fsb2C7hxQ0\",\n",
        "        \"https://source.unsplash.com/CiUR8zISX60\",\n",
        "        \"https://source.unsplash.com/uVnRa6mOLOM\",\n",
        "        \"https://source.unsplash.com/DJ7bWa-Gwks\"]\n",
        "\n",
        "images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\n",
        "images_resized = feature_extractor(images=images, return_tensors=\"pt\")\n",
        "for image in images:\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "X0yNM554ugGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "PlJtkbN3t3ra"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9K4NFGR50l7"
      },
      "source": [
        "# We can download one of the versions of ResNet through tensorflow\n",
        "resnet = tf.keras.applications.ResNet50V2()\n",
        "# Get ready for a bunch of output -- you can see all the layers, in all their glory\n",
        "print(resnet.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9tPegGHwxHR"
      },
      "source": [
        "# There are 64 7x7 kernels in the first conv layer; let's look at them\n",
        "first_conv_layer = resnet.layers[2]\n",
        "weights = first_conv_layer.weights[0]\n",
        "print(\"First conv layer weights shape:\", weights.shape)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for kernel_id in range(weights.shape[-1]):\n",
        "  kernel = np.float32(weights[..., kernel_id])\n",
        "  kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
        "  plt.subplot(8, 8, kernel_id+1)\n",
        "  plt.imshow(kernel)\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_pytorch = images_resized['pixel_values']\n",
        "images_tensorflow = tf.transpose(images_pytorch.detach().numpy(), [0, 2, 3, 1])"
      ],
      "metadata": {
        "id": "eYkY4SIDvUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kkGSgs40dTj"
      },
      "source": [
        "# We can pass the images through the conv layer directly\n",
        "conv_outputs = first_conv_layer(images_tensorflow)\n",
        "print('Output from first conv layer:', conv_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRyIHPR907bH"
      },
      "source": [
        "# We can visualize each of these channels, which are the convolution of the image with the corresponding [7, 7, 3] kernel\n",
        "inches_per_subplot = 2.5\n",
        "for kernel_id in range(16):\n",
        "  kernel = np.float32(first_conv_layer.weights[0][..., kernel_id])\n",
        "  kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
        "  plt.figure(figsize=((len(images)+1)*inches_per_subplot, inches_per_subplot))\n",
        "  plt.subplot(1, len(images)+1, 1)\n",
        "  plt.imshow(kernel)\n",
        "  plt.axis('off')\n",
        "  plt.title(f'Kernel {kernel_id}', fontsize=16)\n",
        "  for image_id in range(len(images)):\n",
        "    plt.subplot(1, len(images)+1, image_id+2)\n",
        "    plt.imshow(conv_outputs[image_id, ..., kernel_id])\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2fNKad13iEC"
      },
      "source": [
        "# All right, here's the cool part, we're going to compute gradients of the \n",
        "# activation with respect to the input image\n",
        "\n",
        "# To do so, we first need to turn the images into a tf.Variable so that it's\n",
        "# automatically tracked by tensorflow's GradientTape\n",
        "images_tensorflow_variable = tf.Variable(images_tensorflow)\n",
        "for kernel_id in range(16):\n",
        "  kernel = np.float32(first_conv_layer.weights[0][..., kernel_id])\n",
        "  kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min())\n",
        "  plt.figure(figsize=((len(images)+1)*inches_per_subplot, inches_per_subplot))\n",
        "  plt.subplot(1, len(images)+1, 1)\n",
        "  plt.imshow(kernel)\n",
        "  plt.title(f'Kernel {kernel_id}', fontsize=16)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    kernel_activation = tf.square(first_conv_layer(images_tensorflow_variable)[..., kernel_id])\n",
        "    kernel_activation = tf.reduce_sum(kernel_activation)\n",
        "  grads = np.float32(tape.gradient(kernel_activation, images_tensorflow_variable))\n",
        "  # Note the gradient is the same size as the input image, and the color of the gradient is relevant\n",
        "  grads = (grads - grads.min()) / (grads.max() - grads.min())\n",
        "  for image_id in range(6):\n",
        "    plt.subplot(1, len(images)+1, image_id+2)\n",
        "    plt.imshow(grads[image_id])\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYqkb4vjKMgZ"
      },
      "source": [
        "# We are going to dig into more than the first layer, so we'll make a new model\n",
        "# which is easily defined using the input to the resnet and the output of one of\n",
        "# the layers\n",
        "# For simplicity, let's just consider the conv layers of the resnet\n",
        "conv_inds = np.where([l.name[-4:]=='conv' for l in resnet.layers])[0]\n",
        "print(\"Indices of conv layers:\", conv_inds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8-ZRk9UKz3w"
      },
      "source": [
        "layer_id = 23\n",
        "mini_model = tf.keras.Model(resnet.input, resnet.layers[layer_id].output)\n",
        "\n",
        "intermed_outp = mini_model(images_tensorflow_variable)\n",
        "print('Intermediate output shape:', intermed_outp.shape[1:])\n",
        "for kernel_id in range(16):  # Just look at the first 16 features of this layer\n",
        "  plt.figure(figsize=((len(images)+1)*inches_per_subplot, inches_per_subplot))\n",
        "  for image_id in range(len(images)):\n",
        "    plt.subplot(1, len(images), image_id+1)\n",
        "    image = np.float32(intermed_outp[image_id, ..., kernel_id])\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnwwDuwxKMXB"
      },
      "source": [
        "# What are the gradients with respect to these later activations?\n",
        "for kernel_id in range(16):\n",
        "  with tf.GradientTape() as tape:\n",
        "    kernel_activation = tf.square(mini_model(images_tensorflow_variable)[..., kernel_id])\n",
        "    kernel_activation = tf.reduce_sum(kernel_activation)\n",
        "  grads = np.float32(tape.gradient(kernel_activation, images_tensorflow_variable))\n",
        "  plt.figure(figsize=(len(images)*inches_per_subplot, inches_per_subplot))\n",
        "  for image_id in range(len(images)):\n",
        "    plt.subplot(1, len(images), image_id+1)\n",
        "    grad_image = grads[image_id]\n",
        "    grad_image = (grad_image - grad_image.min()) / (grad_image.max() - grad_image.min())\n",
        "    plt.imshow(grad_image)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnasSMHnJyJ1"
      },
      "source": [
        "# What if we perturbed the original pixels in the direction of these gradients?\n",
        "# And then, recompute the gradients and perturb the new image to maximize a specific activation\n",
        "\n",
        "# We'll do a little better and use an adaptive optimizer\n",
        "# Also rather than just boosting the activation values, we want the spread to be\n",
        "# boosted so that it's clear where the boosting is happening\n",
        "# (but definitely mess around with all of this, try different things)\n",
        "\n",
        "update_step_size = 1e-3\n",
        "num_perturb_steps = 500\n",
        "moment_power = 2\n",
        "inches_per_subplot = 5\n",
        "for kernel_id in range(8):\n",
        "  images_tensorflow_adj = tf.Variable(images_tensorflow)\n",
        "  opt = tf.keras.optimizers.Adam(update_step_size)\n",
        "  for update_step in range(num_perturb_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "      kernel_activation = mini_model(images_tensorflow_adj)[..., kernel_id]\n",
        "      kernel_activation = tf.reshape(kernel_activation, [len(images), -1])\n",
        "      kernel_activation_exp = tf.pow(kernel_activation, moment_power)\n",
        "      loss = -tf.reduce_sum(tf.reduce_mean(kernel_activation_exp, axis=-1) - tf.pow(tf.reduce_mean(kernel_activation, axis=-1), moment_power))\n",
        "    grads = np.float32(tape.gradient(loss, images_tensorflow_adj))\n",
        "    opt.apply_gradients(zip([grads], [images_tensorflow_adj]))\n",
        "    images_tensorflow_adj.assign(tf.clip_by_value(images_tensorflow_adj, -2.5, 2.5))\n",
        "  plt.figure(figsize=(len(images)*inches_per_subplot, inches_per_subplot))\n",
        "  for image_id in range(len(images)):\n",
        "    plt.subplot(1, len(images), image_id+1)\n",
        "    image = np.float32(images_tensorflow_adj[image_id])\n",
        "    image = (image-image.min())/(image.max()-image.min())\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision transformer"
      ],
      "metadata": {
        "id": "stzemb85zZZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We're downloading a self supervised vision transformer, meaning it didn't actually use any labels, just prior knowledge about augmentations\n",
        "## Let's visualize the attention maps for the images\n",
        "config = ViTConfig.from_pretrained('facebook/dino-vits8', output_hidden_states=True, output_attentions=True)\n",
        "model = ViTModel.from_pretrained('facebook/dino-vits8', config=config)\n",
        "\n",
        "outputs = model(**images_resized)"
      ],
      "metadata": {
        "id": "Ly2UZnVrzbKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the attention maps.  There are 12 layers, with 6 attention heads \n",
        "# each. Then there are 28x28 = 784 patches per image, plus one placeholder token\n",
        "# out front, and you have attention for every token to every other token (785x785)\n",
        "[thing.shape for thing in outputs['attentions']]"
      ],
      "metadata": {
        "id": "k0PxZkk62MII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inches_per_subplot = 2\n",
        "num_attention_heads = 6\n",
        "num_layers = 12\n",
        "for image_ind in range(len(images)):\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  plt.imshow(images[image_ind])\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(num_layers*inches_per_subplot, num_attention_heads*inches_per_subplot))\n",
        "  for i in range(num_layers):\n",
        "    for j in range(num_attention_heads):\n",
        "      plt.subplot(num_attention_heads, num_layers, i+j*num_layers+1)\n",
        "      img = outputs['attentions'][i][image_ind, j].detach().numpy()\n",
        "      img = img[0][1:].reshape([28, 28])\n",
        "      ## upsample\n",
        "      img = tf.image.resize(tf.reshape(img, [1, 28, 28, 1]), [224, 224], method='nearest')\n",
        "      plt.imshow(img[0, ..., 0])\n",
        "      \n",
        "      if j==0:\n",
        "        plt.title(f'Layer {i+1}', fontsize=18)\n",
        "      if i==0:\n",
        "        plt.ylabel(f'Attn head {j+1}', fontsize=18)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  print()"
      ],
      "metadata": {
        "id": "dTCIYrRy5Bk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vpvmt8m75FBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}