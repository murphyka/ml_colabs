{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human learning vs Machine learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hello!\n",
        "\n",
        "The goal of this colab is to build an understanding of what machine learning is, stripped down to a problem where a human (you!) could accomplish the same task given enough brainpower and time.\n",
        "\n",
        "In this colab you will create a model to classify handwritten digits (0-9) from the classic [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). \n",
        "\n",
        "**Human learning:** First the tuning of the model will be done by you. Using a handful of image features describing the dominant line angles in each image, you will try to find the best weighting for each feature to separate the digit classes.\n",
        "\n",
        "**Machine learning:** Next we will let a machine do the same task, faster better stronger, but still the same task.  \n",
        "\n",
        "In both settings -- human learning and machine learning -- we used a machine to help with the computation by speeding through thousands of simple calculations.  The difference between the two settings, and the real power of machine learning, is from when the machine also does the tuning itself."
      ],
      "metadata": {
        "id": "hg11VR-Epdjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DC6xZtebnDVe"
      },
      "outputs": [],
      "source": [
        "#@title python imports\n",
        "!pip install panel --upgrade\n",
        "!pip install ipywidgets_bokeh\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn import svm\n",
        "from IPython.display import clear_output\n",
        "from skimage.transform import hough_line\n",
        "from matplotlib.figure import Figure\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "import panel as pn\n",
        "pn.extension('ipywidgets')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "**Data and model parameters**\n",
        "\n",
        "Decide which digits to try to classify.  We recommend starting with just 0 and 1, to keep it easy.  \n",
        "\n",
        "The second parameter is the number of line angles to detect in each image.  With 4 line angles (the default), there will be 4 values for every image, which we will call \"features\".  The first value will be the presence of 45 degree lines in the image (detected by a Hough transform if you're interested).  The second feature value will measure the presence of vertical lines, then 135 degrees, and finally horizontal lines.  More line angles to detect mean more values to describe each image.\n",
        "\n",
        "We also include a feature that counts the number of \"on\" pixels in each image, represented by an uppercase Σ (sigma, for summation)."
      ],
      "metadata": {
        "id": "Gxnd23ZNs37d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits_to_use = \"0,1\" #@param {type:\"string\"}\n",
        "number_line_angles =  4#@param {type:\"integer\"}\n",
        "\n",
        "digits_to_use = sorted([int(val) for val in digits_to_use.strip().split(',')]) # a hacky way to get a list of ints from an input form"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H0X02Cm0d1aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "We load the dataset and do some preprocessing of the images.  We'll only use a thousand images for the training set.\n",
        "\n",
        "**If you change either of the parameters above, you'll need to re-run the following cell**"
      ],
      "metadata": {
        "id": "jZ9J_2VLt2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the digit data and process into features\n",
        "def preprocess(example):\n",
        "  return tf.image.convert_image_dtype(tf.squeeze(example['image']), tf.float32), example['label']\n",
        "\n",
        "def filter_digits(image, label):\n",
        "  return tf.math.reduce_any(tf.equal(label, tf.constant(digits_to_use, dtype=tf.int64)))\n",
        "\n",
        "dataset_train = tfds.load('mnist', split='train').map(preprocess).filter(filter_digits)\n",
        "dataset_test = tfds.load('mnist', split='test').map(preprocess).filter(filter_digits)\n",
        "\n",
        "number_train_images = 1024\n",
        "number_test_images = 128\n",
        "train_images, train_labels = next(iter(dataset_train.batch(number_train_images)))\n",
        "test_images, test_labels = next(iter(dataset_test.batch(number_test_images)))\n",
        "\n",
        "_, image_height, image_width = train_images.shape\n",
        "\n",
        "tested_line_angles = np.linspace(-np.pi / 2, np.pi / 2, number_line_angles, endpoint=False)\n",
        "train_features = np.concatenate([np.sum(train_images, axis=(1, 2)).reshape([-1, 1]),\n",
        "                          np.stack([np.max(hough_line(train_images[i].numpy(), theta=tested_line_angles)[0], axis=0) for i in range(number_train_images)], 0)\n",
        "                     ], -1)\n",
        "\n",
        "test_features = np.concatenate([np.sum(test_images, axis=(1, 2)).reshape([-1, 1]),\n",
        "                          np.stack([np.max(hough_line(test_images[i].numpy(), theta=tested_line_angles)[0], axis=0) for i in range(number_test_images)], 0)\n",
        "                     ], -1)\n",
        "\n",
        "train_features = (train_features - np.min(train_features, axis=0, keepdims=True)) / (np.max(train_features, axis=0, keepdims=True) - np.min(train_features, axis=0, keepdims=True))\n",
        "test_features = (test_features - np.min(test_features, axis=0, keepdims=True)) / (np.max(test_features, axis=0, keepdims=True) - np.min(test_features, axis=0, keepdims=True))"
      ],
      "metadata": {
        "id": "qid9qhHGLeKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**The model is simple:** for each image, take its feature values and multiply by a set of weights that indicate if it is a `0`.  Then multiply by another set of weights that indicate if it is a `1`, and so on, until you have a value for each possible digit.  The digit with the largest value is the one you classify it as.  \n",
        "\n",
        "The code below shows a randomly selected image from each possible class, along with that image's feature values and the classification output (as probabilities) for the current model. \n",
        "\n",
        "Below this are the weights for each feature, for each digit.  Tune them so that they are high when the feature is usually high (for example, the vertical line feature in `1`'s is usually pretty strong, so we'd increase that weight), and low when the feature is usually low.  The most important features are those that vary a lot between the classes: look for the strongest effect there."
      ],
      "metadata": {
        "id": "hKD_mZOoxnyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tune a model to classify the digits!\n",
        "color_brewer_colors = ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']  ## ty https://colorbrewer2.org/\n",
        "number_features = train_features.shape[1]\n",
        "if number_line_angles == 4:\n",
        "  feature_labels = ['Σ', '--', '\\\\', '|',  '/']\n",
        "  feature_colors = color_brewer_colors[1::2]\n",
        "elif number_features<=10:\n",
        "  feature_labels = ['Σ'] + [f'{np.rad2deg(rad):.1f}' for rad in tested_line_angles]\n",
        "  feature_colors = color_brewer_colors[:number_features]\n",
        "else:\n",
        "  feature_labels = ['']*number_features\n",
        "  feature_colors = ['#e41a1c']*number_features\n",
        "\n",
        "def display_rand_images():\n",
        "  inches_per_subplot = 2.5\n",
        "  number_features = train_features.shape[-1]\n",
        "  num_classes = len(digits_to_use)\n",
        "  image_inds = [np.random.choice(np.where(train_labels==digit)[0]) for digit in digits_to_use]\n",
        "\n",
        "  class_weights = []\n",
        "  for digit_ind, digit in enumerate(digits_to_use):\n",
        "    feature_weights = [slider.value for slider in sliders[digit_ind][::-1]]\n",
        "    class_weights.append(np.sum(np.reshape(feature_weights, [1, -1])*train_features, axis=-1))  ## Shape [N], one scalar per image\n",
        "  class_weights = np.stack(class_weights, -1)  ## Shape [N, num_digits]\n",
        "  # Simplify the fitting situation by solving for biases that even out the class predictions (so that 1/num_classes are positive and the rest neg for each class weighting)\n",
        "  biases = np.percentile(class_weights, 100*(1-1./len(digits_to_use)), axis=0)\n",
        "  class_weights -= biases.reshape([1, -1])\n",
        "  assigned_probs = tf.nn.softmax(class_weights, axis=1)\n",
        "  acc = np.mean(np.int32(digits_to_use)[np.argmax(assigned_probs, axis=1)] == train_labels)\n",
        "\n",
        "  fig = Figure(figsize=(3*inches_per_subplot*1.4, inches_per_subplot*len(digits_to_use)))\n",
        "  specs = gridspec.GridSpec(ncols=3, nrows=len(digits_to_use), figure=fig)\n",
        "  for plot_ind, image_ind in enumerate(image_inds):\n",
        "    ax = fig.add_subplot(specs[plot_ind, 0])\n",
        "    ax.imshow(train_images[image_ind], cmap='gray_r')\n",
        "    ax.axis('off')\n",
        "\n",
        "    ax = fig.add_subplot(specs[plot_ind, 1])\n",
        "    ax.barh(np.arange(number_features), train_features[image_ind], color=feature_colors)\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.set_yticks(range(number_features))\n",
        "    ax.set_yticklabels(feature_labels, fontsize=16)\n",
        "    ax.set_xticks([])\n",
        "    if not plot_ind:\n",
        "      ax.set_title('Image features', fontsize=14)\n",
        "\n",
        "    ax = fig.add_subplot(specs[plot_ind, 2])\n",
        "\n",
        "    ax.bar(np.arange(num_classes), assigned_probs[image_ind], color='#798086')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.set_xticks(range(num_classes))\n",
        "    ax.set_xticklabels(digits_to_use, fontsize=16)\n",
        "    ax.set_yticks([0, 0.5, 1.])\n",
        "    if not plot_ind:\n",
        "      ax.set_title('Assigned probability', fontsize=14)\n",
        "  return fig\n",
        "\n",
        "def train_svm(_):\n",
        "  svm_instance = svm.LinearSVC(max_iter=10_000)\n",
        "  svm_instance.fit(train_features, train_labels)\n",
        "  coeffs = svm_instance.coef_\n",
        "  ## If there are only 2 classes, then there's only 1 weight vector; just duplicate it (negated) for the second class\n",
        "  if len(digits_to_use) == 2:\n",
        "    for slider_ind, slider in enumerate(sliders[0]):\n",
        "      slider.value = -coeffs[0, len(sliders[0])-1-slider_ind]\n",
        "    for slider_ind, slider in enumerate(sliders[1]):\n",
        "      slider.value = coeffs[0, len(sliders[1])-1-slider_ind]\n",
        "  else:\n",
        "    for digit_ind, digit in enumerate(digits_to_use):\n",
        "      for slider_ind, slider in enumerate(sliders[digit_ind]):\n",
        "        slider.value = coeffs[digit_ind, len(sliders[digit_ind])-1-slider_ind]\n",
        "  return\n",
        "\n",
        "def refresh_images(_):\n",
        "  mpl_pane.object = display_rand_images()\n",
        "\n",
        "def evaluate_model(_):\n",
        "  class_weights = []\n",
        "  for digit_ind, digit in enumerate(digits_to_use):\n",
        "    feature_weights = [slider.value for slider in sliders[digit_ind][::-1]]\n",
        "    class_weights.append(np.sum(np.reshape(feature_weights, [1, -1])*train_features, axis=-1))  ## Shape [N], one scalar per image\n",
        "  class_weights = np.stack(class_weights, -1)  ## Shape [N, num_digits]\n",
        "  # Simplify the fitting situation by solving for biases that even out the class predictions (so that 1/num_classes are positive and the rest neg for each class weighting)\n",
        "  biases = np.percentile(class_weights, 100*(1-1./len(digits_to_use)), axis=0)\n",
        "  class_weights -= biases.reshape([1, -1])\n",
        "  assigned_probs = tf.nn.softmax(class_weights, axis=-1)\n",
        "  acc = np.mean(np.int32(digits_to_use)[np.argmax(assigned_probs, axis=1)] == train_labels)\n",
        "  accuracy_text.value = f'Accuracy: {acc:.4f}'\n",
        "  return\n",
        "\n",
        "\n",
        "evaluate_button = pn.widgets.Button(name=\"Evaluate model\", button_type='primary')\n",
        "refresh_button = pn.widgets.Button(name=\"Refresh images\", button_type='success')\n",
        "accuracy_text = pn.widgets.TextInput(value='')\n",
        "evaluate_bundle = pn.Row(evaluate_button, accuracy_text)\n",
        "ml_button = pn.widgets.Button(name=\"MACHINE LEARNING\", button_type='danger')\n",
        "\n",
        "sliders = [[pn.widgets.FloatSlider(name=feature_labels[feature_ind], value=0, start=-5, end=5, step=0.05, bar_color=feature_colors[feature_ind]) for feature_ind in range(number_features)[::-1]] for digit in digits_to_use]\n",
        "mpl_pane = pn.pane.Matplotlib(display_rand_images(), dpi=72, tight=True)\n",
        "\n",
        "evaluate_button.on_click(evaluate_model)\n",
        "ml_button.on_click(train_svm)\n",
        "refresh_button.on_click(refresh_images)\n",
        "items = []\n",
        "for thing in zip(digits_to_use, [pn.Column(*slider_bunch, background='WhiteSmoke', width=200) for slider_bunch in sliders]):\n",
        "  items.append([f'WEIGHTS FOR {thing[0]}'] + thing[1])\n",
        "evaluate_model('')\n",
        "\n",
        "display(mpl_pane)\n",
        "display(refresh_button)\n",
        "\n",
        "grid = pn.GridBox(*items, ncols=len(digits_to_use))\n",
        "display(grid)\n",
        "display(evaluate_bundle)\n",
        "display(ml_button)"
      ],
      "metadata": {
        "id": "AZAQ7lXX-_oX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Finally, since machine learning can sift through tons of features without caring what they mean, we can even hand in completely random features.  It turns out [random projection](https://en.wikipedia.org/wiki/Random_projection) can be a pretty powerful method of dimensionality reduction.  How well does it do to classify digits?  You may want to re-sample the features a few times, and see how much the performance changes.\n",
        "\n",
        "**How much more difficult is it to tell what's been learned?**"
      ],
      "metadata": {
        "id": "gOb-K8gM5dtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits_to_use = \"0,1,2,3\" #@param {type:\"string\"}\n",
        "number_random_features =  40#@param {type:\"integer\"}\n",
        "\n",
        "digits_to_use = sorted([int(val) for val in digits_to_use.strip().split(',')]) # a hacky way to get a list of ints from an input form"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7U6PXJlK6B5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reprocess into features (and resample the random projections)\n",
        "dataset_train = tfds.load('mnist', split='train').map(preprocess).filter(filter_digits)\n",
        "dataset_test = tfds.load('mnist', split='test').map(preprocess).filter(filter_digits)\n",
        "\n",
        "train_images, train_labels = next(iter(dataset_train.batch(number_train_images)))\n",
        "test_images, test_labels = next(iter(dataset_test.batch(number_test_images)))\n",
        "\n",
        "_, image_height, image_width = train_images.shape\n",
        "\n",
        "random_vectors = np.random.normal(size=(number_random_features, 28*28))\n",
        "train_features = np.dot(np.reshape(train_images, [-1, 28*28]), random_vectors.T)\n",
        "test_features = np.dot(np.reshape(test_images, [-1, 28*28]), random_vectors.T)\n",
        "\n",
        "train_features = (train_features - np.min(train_features, axis=0, keepdims=True)) / (np.max(train_features, axis=0, keepdims=True) - np.min(train_features, axis=0, keepdims=True))\n",
        "test_features = (test_features - np.min(test_features, axis=0, keepdims=True)) / (np.max(test_features, axis=0, keepdims=True) - np.min(test_features, axis=0, keepdims=True))"
      ],
      "metadata": {
        "id": "aUC3XdGk6B5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random feature model!\n",
        "color_brewer_colors = ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']  ## ty https://colorbrewer2.org/\n",
        "\n",
        "number_features = train_features.shape[-1]\n",
        "if number_features <= 10:\n",
        "  feature_colors = color_brewer_colors[:number_features]\n",
        "else:\n",
        "  feature_colors = ['#e41a1c']*number_features\n",
        "feature_labels = ['']*number_features\n",
        "\n",
        "def display_rand_images():\n",
        "  inches_per_subplot = 2.5\n",
        "  num_classes = len(digits_to_use)\n",
        "  image_inds = [np.random.choice(np.where(train_labels==digit)[0]) for digit in digits_to_use]\n",
        "\n",
        "  class_weights = []\n",
        "  for digit_ind, digit in enumerate(digits_to_use):\n",
        "    feature_weights = [slider.value for slider in sliders[digit_ind][::-1]]\n",
        "    class_weights.append(np.sum(np.reshape(feature_weights, [1, -1])*train_features, axis=-1))  ## Shape [N], one scalar per image\n",
        "  class_weights = np.stack(class_weights, -1)  ## Shape [N, num_digits]\n",
        "  # Simplify the fitting situation by solving for biases that even out the class predictions (so that 1/num_classes are positive and the rest neg for each class weighting)\n",
        "  biases = np.percentile(class_weights, 100*(1-1./len(digits_to_use)), axis=0)\n",
        "  class_weights -= biases.reshape([1, -1])\n",
        "  assigned_probs = tf.nn.softmax(class_weights, axis=1)\n",
        "  acc = np.mean(np.int32(digits_to_use)[np.argmax(assigned_probs, axis=1)] == train_labels)\n",
        "\n",
        "  fig = Figure(figsize=(3*inches_per_subplot*1.4, inches_per_subplot*len(digits_to_use)))\n",
        "  specs = gridspec.GridSpec(ncols=3, nrows=len(digits_to_use), figure=fig)\n",
        "  for plot_ind, image_ind in enumerate(image_inds):\n",
        "    ax = fig.add_subplot(specs[plot_ind, 0])\n",
        "    ax.imshow(train_images[image_ind], cmap='gray_r')\n",
        "    ax.axis('off')\n",
        "\n",
        "    ax = fig.add_subplot(specs[plot_ind, 1])\n",
        "    ax.barh(np.arange(number_features), train_features[image_ind], color=feature_colors)\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.set_yticks(range(number_features))\n",
        "    ax.set_yticklabels(feature_labels, fontsize=16)\n",
        "    ax.set_xticks([])\n",
        "    if not plot_ind:\n",
        "      ax.set_title('Image features', fontsize=14)\n",
        "\n",
        "    ax = fig.add_subplot(specs[plot_ind, 2])\n",
        "\n",
        "    ax.bar(np.arange(num_classes), assigned_probs[image_ind], color='#798086')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.set_xticks(range(num_classes))\n",
        "    ax.set_xticklabels(digits_to_use, fontsize=16)\n",
        "    ax.set_yticks([0, 0.5, 1.])\n",
        "    if not plot_ind:\n",
        "      ax.set_title('Assigned probability', fontsize=14)\n",
        "  return fig\n",
        "\n",
        "def train_svm(_):\n",
        "  svm_instance = svm.LinearSVC(max_iter=10_000)\n",
        "  svm_instance.fit(train_features, train_labels)\n",
        "  coeffs = svm_instance.coef_\n",
        "  ## If there are only 2 classes, then there's only 1 weight vector; just duplicate it (negated) for the second class\n",
        "  if len(digits_to_use) == 2:\n",
        "    for slider_ind, slider in enumerate(sliders[0]):\n",
        "      slider.value = -coeffs[0, len(sliders[0])-1-slider_ind]\n",
        "    for slider_ind, slider in enumerate(sliders[1]):\n",
        "      slider.value = coeffs[0, len(sliders[1])-1-slider_ind]\n",
        "  else:\n",
        "    for digit_ind, digit in enumerate(digits_to_use):\n",
        "      for slider_ind, slider in enumerate(sliders[digit_ind]):\n",
        "        slider.value = coeffs[digit_ind, len(sliders[digit_ind])-1-slider_ind]\n",
        "  return\n",
        "\n",
        "def refresh_images(_):\n",
        "  mpl_pane.object = display_rand_images()\n",
        "\n",
        "def evaluate_model(_):\n",
        "  class_weights = []\n",
        "  for digit_ind, digit in enumerate(digits_to_use):\n",
        "    feature_weights = [slider.value for slider in sliders[digit_ind][::-1]]\n",
        "    class_weights.append(np.sum(np.reshape(feature_weights, [1, -1])*train_features, axis=-1))  ## Shape [N], one scalar per image\n",
        "  class_weights = np.stack(class_weights, -1)  ## Shape [N, num_digits]\n",
        "  # Simplify the fitting situation by solving for biases that even out the class predictions (so that 1/num_classes are positive and the rest neg for each class weighting)\n",
        "  biases = np.percentile(class_weights, 100*(1-1./len(digits_to_use)), axis=0)\n",
        "  class_weights -= biases.reshape([1, -1])\n",
        "  assigned_probs = tf.nn.softmax(class_weights, axis=-1)\n",
        "  acc = np.mean(np.int32(digits_to_use)[np.argmax(assigned_probs, axis=1)] == train_labels)\n",
        "  accuracy_text.value = f'Accuracy: {acc:.4f}'\n",
        "  return\n",
        "\n",
        "\n",
        "evaluate_button = pn.widgets.Button(name=\"Evaluate model\", button_type='primary')\n",
        "refresh_button = pn.widgets.Button(name=\"Refresh images\", button_type='success')\n",
        "accuracy_text = pn.widgets.TextInput(value='')\n",
        "evaluate_bundle = pn.Row(evaluate_button, accuracy_text)\n",
        "ml_button = pn.widgets.Button(name=\"MACHINE LEARNING\", button_type='danger')\n",
        "\n",
        "sliders = [[pn.widgets.FloatSlider(name=feature_labels[feature_ind], value=0, start=-5, end=5, step=0.05, bar_color=feature_colors[feature_ind]) for feature_ind in range(number_features)[::-1]] for digit in digits_to_use]\n",
        "mpl_pane = pn.pane.Matplotlib(display_rand_images(), dpi=72, tight=True)\n",
        "\n",
        "evaluate_button.on_click(evaluate_model)\n",
        "ml_button.on_click(train_svm)\n",
        "refresh_button.on_click(refresh_images)\n",
        "items = []\n",
        "for thing in zip(digits_to_use, [pn.Column(*slider_bunch, background='WhiteSmoke', width=200) for slider_bunch in sliders]):\n",
        "  items.append([f'WEIGHTS FOR {thing[0]}'] + thing[1])\n",
        "evaluate_model('')\n",
        "\n",
        "display(mpl_pane)\n",
        "display(refresh_button)\n",
        "\n",
        "grid = pn.GridBox(*items, ncols=len(digits_to_use))\n",
        "display(grid)\n",
        "display(evaluate_bundle)\n",
        "display(ml_button)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fgzcVB6F6B5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When interpretability is difficult, it's important to have other checks that the model is performing as well as you think it is.  If you tried a bunch of random projections in the cells above, you might have found some that work particularly well with the specific set of training images, but wouldn't work well on unseen images.  This is the problem of **overfitting**.\n",
        "\n",
        "Evaluate the accuracy on an unseen test set below, to see if it matches the accuracy on the training set."
      ],
      "metadata": {
        "id": "MqEL02grOzc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate on the test set\n",
        "class_weights = []\n",
        "for digit_ind, digit in enumerate(digits_to_use):\n",
        "  feature_weights = [slider.value for slider in sliders[digit_ind][::-1]]\n",
        "  class_weights.append(np.sum(np.reshape(feature_weights, [1, -1])*test_features, axis=-1))  ## Shape [N], one scalar per image\n",
        "class_weights = np.stack(class_weights, -1)  ## Shape [N, num_digits]\n",
        "# Simplify the fitting situation by solving for biases that even out the class predictions (so that 1/num_classes are positive and the rest neg for each class weighting)\n",
        "biases = np.percentile(class_weights, 100*(1-1./len(digits_to_use)), axis=0)\n",
        "class_weights -= biases.reshape([1, -1])\n",
        "assigned_probs = tf.nn.softmax(class_weights, axis=-1)\n",
        "acc = np.mean(np.int32(digits_to_use)[np.argmax(assigned_probs, axis=1)] == test_labels)\n",
        "print(f'Test set accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SesTL_HL6bLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TqaHh9LGNOBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
